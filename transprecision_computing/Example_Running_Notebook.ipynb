{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./lagrangian-dual-deep-learning/transprecision_computing/')\n",
    "from dataset import *\n",
    "from Regressor import   *\n",
    "from SBRregressor import  *\n",
    "from SBRregressor2 import  *\n",
    "from settings import *\n",
    "from util import *\n",
    "import argparse\n",
    "import torch\n",
    "Ten = torch.FloatTensor\n",
    "iTen = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /lagrangian-dual-deep-learning/transprecision_computing/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Specify main parameters for running experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seed = 0\n",
    "violated_const_ratio = 0.5 # you can adjust this number to play around \n",
    "start_point_seed = 2\n",
    "\n",
    "benchmark = 'convolution'\n",
    "\n",
    "params = {'epochs': 150,\n",
    "               'n_data': 4000,\n",
    "               'batch_size': 256,\n",
    "               'violated_const_ratio': violated_const_ratio,  # this is used to create a trainig set with a specific\n",
    "               'benchmark': benchmark,\n",
    "               'split': [0.5, 0.25, 0.25],\n",
    "               'seed': test_seed}\n",
    "\n",
    "d_trainall = Dataset(params, 'train', 'cpu')\n",
    "d_test = Dataset(params, 'test', 'cpu')\n",
    "d_valall = Dataset(params, 'valid', 'cpu')\n",
    "res['d_test'] = d_test\n",
    "X_test, y_test = d_test._dataset\n",
    "train_size = 2000 # more training size models will be more accurate\n",
    "val_size = 600 \n",
    "split_seed = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get train/val dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above defined parameters, we then  can have train/ validation dataset\n",
    "# In our experiments, we fixed the test set\n",
    "# We increase the training size gradually to see its impact \n",
    "\n",
    "X_train,y_train  = copy.deepcopy( d_trainall._dataset)\n",
    "idx_train = np.random.choice( list(range(len(X_train))), train_size)\n",
    "X_train, y_train = X_train[idx_train,:], y_train[idx_train]\n",
    "\n",
    "d_train = copy.deepcopy(d_trainall)\n",
    "d_train._dataset = (X_train, y_train)\n",
    "y_med_pred = np.median(y_train)* np.ones(len(y_test))\n",
    "\n",
    "X_val, y_val = copy.deepcopy(d_valall._dataset)\n",
    "idx_val = np.random.choice(list(range(len(X_val))), val_size)\n",
    "X_val, y_val = X_val[idx_val,:], y_val[idx_val]\n",
    "d_val = copy.deepcopy(d_valall)\n",
    "d_val._dataset = (X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of a simple regressor \n",
      "MAE = 0.0747935026884079, Constraint Violation = 198\n",
      "-------------------------\n",
      "Performance of a constrained regressor, but we fix the multiplier during the course of training \n",
      "MAE = 0.13169240951538086, Constraint Violation = 0\n",
      "-------------------------\n",
      "Performance of a constrained regressor, using single/common multiplier for all constraints, updating the multiplier by epochs\n",
      "MAE = 0.09749921411275864, Constraint Violation = 4\n",
      "-------------------------\n",
      "Constrained Regressor, using different multipliers for all constraints, updating all multipliers by epochs\n",
      "MAE = 0.07603932917118073, Constraint Violation = 17\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# A simple regressor without constraints\n",
    "model_1 = Regressor(params, d_train, d_test, d_val,  start_point_seed) # start_point_seed is random seed of pytorch\n",
    "# make sure all models starting from the same initial points\n",
    "model_1.train()\n",
    "tmp = model_1.test()\n",
    "print('Performance of a simple regressor ')\n",
    "print ('MAE = {}, Constraint Violation = {}'.format(tmp[0], tmp[1]))\n",
    "print('-------------------------')\n",
    "\n",
    "\n",
    "\n",
    "# print('Performance of a simple regressor ')\n",
    "\n",
    "model_2_1 = SBRregressor(params, d_train, d_test, d_val,  start_point_seed)\n",
    "model_2_1.train(options = {'mult_fixed':True})\n",
    "\n",
    "tmp = model_2_1.test()\n",
    "print('Performance of a constrained regressor, but we fix the multiplier during the course of training ')\n",
    "print ('MAE = {}, Constraint Violation = {}'.format(tmp[0], tmp[1]))\n",
    "\n",
    "print('-------------------------')\n",
    "\n",
    "###################regularization with single multiplier updated gradually, starts with 0\n",
    "\n",
    "model_2 = SBRregressor(params, d_train, d_test, d_val,  start_point_seed)\n",
    "if split_seed == 0:\n",
    "    model_2.opt_lr_rate()\n",
    "    best_lr_model_2 = copy.deepcopy( model_2._LR_rate)\n",
    "else:\n",
    "    model_2._LR_rate = copy.deepcopy( best_lr_model_2)\n",
    "    model_2.train(options={'mult_fixed': False})\n",
    "tmp = model_2.test()\n",
    "\n",
    "\n",
    "print('Performance of a constrained regressor, using single/common multiplier for all constraints, updating the multiplier by epochs')\n",
    "print ('MAE = {}, Constraint Violation = {}'.format(tmp[0], tmp[1]))\n",
    "\n",
    "print('-------------------------')\n",
    "\n",
    "\n",
    "\n",
    "########## regularization with a multiplier for each constraint, each multiplier has updated gradually, starts with 0\n",
    "\n",
    "# Note the difference between model_3 vs model_2 is in model_3 we have different multpliers instead of using shared multiplier\n",
    "# as in model_2\n",
    "model_3 = SBRregressor2(params, d_train, d_test, d_val,  start_point_seed)\n",
    "if split_seed == 0:\n",
    "    model_3.opt_lr_rate()\n",
    "    best_lr_model_3 = copy.deepcopy(model_3._LR_rate)\n",
    "else:\n",
    "    model_3._LR_rate = copy.deepcopy( best_lr_model_3)\n",
    "    model_3.train(options={'mult_fixed': False})\n",
    "\n",
    "tmp = model_3.test()\n",
    "\n",
    "\n",
    "print('Constrained Regressor, using different multipliers for all constraints, updating all multipliers by epochs')\n",
    "print ('MAE = {}, Constraint Violation = {}'.format(tmp[0], tmp[1]))\n",
    "\n",
    "print('-------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
